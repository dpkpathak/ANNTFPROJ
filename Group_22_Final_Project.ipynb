{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1PmouLwIuzmm"
   },
   "source": [
    "![alt text](https://www.signavio.com/wp-content/uploads/2013/03/Logo-Universitaet-Osnabrueck-300x99.png)\n",
    "\n",
    "\n",
    "**PROJECT REPORT**\n",
    "\n",
    "*Implementing ANNs with Tensorflow  (8.3304)*\n",
    "\n",
    "\n",
    "**Double DQN (Hasselt et al. 2015)**\n",
    "\n",
    "Tensorflow Implementation for Solving OpenAI Gym Environments\n",
    "\n",
    "**Participants**\n",
    "\n",
    "Saurabh Mishra, Deepak Pathak, Krupal Shah, Christian Johnson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wnMsQA6KQmPm"
   },
   "source": [
    "### Imports and Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RAD4l9T84EFA"
   },
   "source": [
    "Import git repository into current session using below command to download code and trained model\n",
    "\n",
    "`!git clone https://github.com/dpkpathak/ANNTFPROJ.git`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "colab_type": "code",
    "id": "uRaCwT0S4W81",
    "outputId": "6f17182a-2bbb-4458-81ab-5046abfc8d35"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/dpkpathak/ANNTFPROJ.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q9mKjy3y4fvZ"
   },
   "source": [
    "Change current working directory to the git repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4SCYYwf94ref"
   },
   "outputs": [],
   "source": [
    "!cd ANNTFPROJ/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bwwvaDBrtTpq"
   },
   "source": [
    "### Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FHPlfXXxwInV"
   },
   "source": [
    "In this project we aimed to implement the Double Deep Q Network (DDQN) reinforcement learning algorithm proposed by Hasselt et al. in their 2015 paper. The Q-learning algorithm, which updates Q-value estimations for any given state-action pair via the Bellman equation, and its Neural-Network-based equivalent for more complex environments, Deep Q-learning (DQN), were shown by Hasselt and his colleagues to lead to systematic overestimation of Q-values as a result of single-network Q-value estimation from maximum predicted values. In short, because the primary Q-value estimation network chooses a maximum estimation, in especially noisy environments these overestimations can compound and negatively affect training in specific states, leading to, ultimately, suboptimal policies. In this project we aimed understand the motivations for this algorithm and recreate the comparative improvements shown by Hasselt and his colleagues. Thus, our report first explores several previous algorithms motivating the main implementation algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BEhAsnXDtVXG"
   },
   "source": [
    "### Theoretical Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TwVvvpnH_-Kq"
   },
   "source": [
    "#### Basics and Q Learning\n",
    "\n",
    "The DDQN algorithm is motivated by the limitations of several previous algorithms. Thus, we begin with a short overview of these algorithms to motivate our implementation and provide the necessary theoretical background for understanding why the DDQN algorithm is advantageous. \n",
    "\n",
    "The original Q Learning algorithm, first sketched by Watkins in his 1989 PhD thesis, is a reinforcement learning algorithm for controlling Markov decision processes via incremental policy adjustment (Watkins, 1989). In its tabular implementation, an agent learns to effectively estimate Q-values for any state-action pair by updating values in a state-action matrix via the Bellman equation, which uses the weighted average of old values plus information from recent episodes. The below procedure summarizes this algorithm and the Bellman equation.\n",
    "\n",
    "##### Algorithm\n",
    "\n",
    "- Procedure Q-Learning($\\epsilon, \\alpha, \\gamma$):\n",
    "    - Initialize $Q(s,a)$ for all $s\\in S, a \\in A$ arbitrarily except $Q(terminal,\\cdot) = 0$\n",
    "    - $\\pi \\gets \\epsilon$-greedy policy with respect to Q\n",
    "    - **for** each episode **do**\n",
    "        - Set $s_1$ as starting state\n",
    "        - $t \\gets 1$\n",
    "        - **loop** untlil episode terminates\n",
    "            - Sample action $a_t$ from plolicy $\\pi(s_t)$\n",
    "            - Take action $a_t$ and observe reward $r_t$ and next state $s_{t+1}$\n",
    "            - $Q(s_t, a_t) \\gets Q(s_t, a_t) + \\alpha[r_t + \\gamma\\max\\limits_{a'}Q(s_{t+1},a') - Q(s_t, a_t)]$\n",
    "            - $\\pi \\gets \\epsilon$-greedy policy with respect to Q(policy improvement)\n",
    "            - $t \\gets t+1$\n",
    "    - return $Q, \\pi$\n",
    "\n",
    " \n",
    "\n",
    "The cell below shows the results of a tabular implementation of the Q-Learning algorithm for a simple stochastic environment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D_LOHtndkcCv"
   },
   "source": [
    "**Environment detail for this example:**\n",
    "\n",
    "(Adapted from Rubikscode)\n",
    "<img src=\"https://i0.wp.com/rubikscode.net/wp-content/uploads/2020/01/Simple-MDP.png?w=1080&ssl=1\">\n",
    "\n",
    "This is one simple environment with 4 states: X, Y, Z and W. X is starting state , while states Z and W are terminal. There are two actions that agent can take in state X – UP and DOWN. Reward fro taking these actions is 0. The interesting part is the set of actions from state Y to W. The reward for this set of actions follows normal distribution with mean -0.5 and standard deviation 1. This means that after a large number of iterations reward will be negative.\n",
    "\n",
    "This in turn means that our learning agent should never pick the action UP from the state X in the first place, if it wants to minimize the loss, i.e. the goal of the agent would actually be to get reward 0, or the least negative value. This is where Q-Learning has problems. Because we have that specific distribution of reward learning agent can be fooled that it should take action UP in the state X. In a nutshell, max operator updates the Q-Value, which could be positive for this action, learning agent takes this action as valid option. Q-Value is overestimated! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "colab_type": "code",
    "id": "rNwJB9n_AzIZ",
    "outputId": "7fa1876a-fdc7-44b3-9947-d2f561733567"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from q_learn.q_learning import MDP, mdp_q_learning\n",
    "\n",
    "mdp_environment = MDP()\n",
    "q_reward, q_table, num_of_ups = mdp_q_learning(mdp_environment)\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "plt.plot(num_of_ups / 10000 * 100, label='UPs in X', color='red')\n",
    "plt.plot(q_reward, color='blue', label='Reward')\n",
    "plt.legend()\n",
    "plt.ylabel('Percentage of ups in state X')\n",
    "plt.xlabel('Episodes')\n",
    "plt.title(r'Q-Learning')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OVDjNHEy_o2k"
   },
   "source": [
    "#### Double Q Learning\n",
    "\n",
    "One problem with the Q Learning algorithm is that Q-value estimation and action selection are both performed by the same approximation function, in the above case, the estimations produced by combining the Bellman equation with previous state-action table values. Because the maximum Q-value is used for the update (see above equation), in noisy environments the agent will tend to update the policy based on overestimations which ultimately slow training. \n",
    "\n",
    "The solution to this problem, proposed by Hasselt, is to introduce a second approximation function which is randomly updated in deference to the other (Hasselt, 2010). When selecting an action, the approximation function used is separate from that evaluating the action, thus achieving the decoupling required to remove overestimations. The so-called Double Q Learning algorithm is especially effective in noisy environments. The image below summarizes the updated algorithm. Note that either approximation function is updated by random choice and that when updating, the other function is used for value approximation.\n",
    "##### Algorithm\n",
    "\n",
    "- Algorithm Double Q-learning\n",
    "    - Initialize $Q^A, Q^B, s$\n",
    "    - **repeat**\n",
    "        - Choose $a$, based on $Q^A(s, \\cdot)$, observe $r, s'$\n",
    "        - Choose (e.g. random) either *UPDATE(A)* or *UPDATE(B)*\n",
    "        - **if** UPDATE(A) **then**\n",
    "            - Define $a^* = \\arg\\max_a Q^A(s',a)$\n",
    "            - $Q^A(s,a) \\gets Q^A + \\alpha(s,a)(r + \\gamma Q^B(s',a^*) - Q^A(s,a))$\n",
    "        - **else if** UPDATE(B) **then**\n",
    "            - Define $b^* = \\arg\\max_a Q^B(s',a)$\n",
    "            - $Q^B(s,a) \\gets Q^B + \\alpha(s,a)(r + \\gamma Q^A(s',a^*) - Q^B(s,a))$\n",
    "        - **end if**\n",
    "        - $s \\gets s'$\n",
    "    - **until** end\n",
    "\n",
    "\n",
    "The cell below shows the results of a tabular implementation of this algorithm in the same stochastic environment. Note that training results are improved versus the original Q-Learning algorithm, shown in the number of times the agent selects the unrewarding 'UP' action. This is because the agent is able to avoid the adverse effects of noise-induced overestimation by distributing experiences across the two decoupled approximation functions. In the following graph you can see the two algorithms compared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RQwrsi_8A_dt"
   },
   "outputs": [],
   "source": [
    "from q_learn.double_q_learning import mdp_double_q_learning\n",
    "\n",
    "mdp_environment = MDP()\n",
    "dq_reward, _, _, dq_num_of_ups = mdp_double_q_learning(mdp_environment)\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "plt.plot(dq_num_of_ups / 10000 * 100, label='UPs in X', color='#FF171A')\n",
    "plt.plot(dq_reward, color='#6C5F66', label='Reward')\n",
    "plt.legend()\n",
    "plt.ylabel('Percentage of UPs in state X')\n",
    "plt.xlabel('Episodes')\n",
    "plt.title(r'Double Q-Learning')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HChcYf2FkwWq"
   },
   "source": [
    "The cell below shows the results side-by-side. The Q-learning agent explores the unrewarding UP action much more than the Double Q-learning agent during initial stages episodes, due to the overestimation of the stochastic rewards for this action which are neatly avoided by the Double Q-learning agent. Thus, although both agents can ultimately reach an optimal policy, the Double Q-learning agent is more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RHLMI0kFj5M0"
   },
   "outputs": [],
   "source": [
    "# Visualizing the choices of both agents for comparison.\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.plot(dq_num_of_ups/10000*100, label='Double Q-learning', color='g')\n",
    "plt.plot(num_of_ups/10000*100, label='Q-learning', color='#FF171A')\n",
    "plt.legend()\n",
    "plt.ylabel('Percentage of UPs in state X')\n",
    "plt.xlabel('Episodes')\n",
    "plt.title(r'Double Q-Learning vs. Q-Learning')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nvkamoccAMk0"
   },
   "source": [
    "### Deep Q Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ae2JsWOZWMVh"
   },
   "source": [
    "In more complex environments, such as OpenAI gym environments, a tabular representation is infeasible because of the sheer amount of state-action pairs. Thus, a more efficient means of updating and representing the agent policy is by introducing a neural network to replace the tabular approximation function. By iteratively training a network on batches of experiences, Mnih et al showed that neural reinforcement learning is an effective update to the original Q-Learning algorithm (Mnih et al, 2015).\n",
    "\n",
    "The cells below demonstrate the Deep Q Learning algorithm proposed by Mnih et al, which is based on the original Q-Learning algorithm. As such, this method reproduces the kinds of overestimation errors suggested by Hasselt and his colleagues. \n",
    "\n",
    "In the following cells we will walk through and explain the relevant sections of the DQN implementation, which will be relevant for the DDQN implementation as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "36YiKNiIQmPi"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import gym\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P_DpeD0UQmPY"
   },
   "outputs": [],
   "source": [
    "# Epsilon greedy policy variables.\n",
    "max_epsilon = 1\n",
    "min_epsilon = 0.01\n",
    "lambda_ = 0.0005\n",
    "\n",
    "# DQN algorithm variables.\n",
    "gamma = 0.95\n",
    "batch_size = 32\n",
    "tau = 0.08\n",
    "max_experiences = 500000\n",
    "min_experiences = 96\n",
    "\n",
    "# Network model variables.\n",
    "hidden_units = [50,50]\n",
    "steps = 0\n",
    "\n",
    "# Environment variables.\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "num_actions = env.action_space.n\n",
    "num_states = len(env.observation_space.sample())\n",
    "num_episodes = 100\n",
    "total_rewards = np.empty(num_episodes)\n",
    "total_loss = np.empty(num_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "svPxBv6tnbVj"
   },
   "outputs": [],
   "source": [
    "# To organize the information for training, we must create an agent which will \n",
    "# iteratively train itself within the openAI gym environment. This is achieved via\n",
    "# an 'Agent' class.\n",
    "\n",
    "class Agent:\n",
    "  \"\"\"\n",
    "  Reinforcement learning agent to implement DQN and DDQN algorithms.\n",
    "  \"\"\"\n",
    "  def get_action(self, state, eps):\n",
    "    pass\n",
    "\n",
    "\n",
    "  def train(self,exp_buffer):\n",
    "    pass\n",
    "\n",
    "\n",
    "  def learn(self):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent has three important functions:\n",
    "\n",
    "The 'get action' function will select the next action the agent should take, \n",
    "given the current state of the environment. In this function we implement\n",
    "the so-called 'epsilon greedy policy', which allows the agent to explore the\n",
    "environment during early episodes, and, as the epsilon value decays, encourage \n",
    "the agent to exploit the environment based on  previous training. To implement\n",
    "this policy there is a random switch which enforces either the explore or \n",
    "exploit strategy based on the decaying epsilon value. We update the epsilon\n",
    "value during the main training loop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(self, state, eps):\n",
    "  if random.random() < eps:\n",
    "    # Explore.\n",
    "    return random.randint(0, num_actions - 1)\n",
    "  else:\n",
    "    # Exploit.\n",
    "    return np.argmax(self.train_net(state.reshape(1, -1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'learn' function houses the main training loop for the agent. Here we do not \n",
    "implement algorithm specifics, but iterate through the episodes and update the \n",
    "epsilon value and relevant experiment variables such as training loss. The \n",
    "relevant section here is the experience update. Found in the while loop of the \n",
    "current episode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "  # First, the agent selects an action using the 'get action' function.\n",
    "  action = self.get_action(state, eps)\n",
    "\n",
    "  # This action influences the rewards and next state of the environment.\n",
    "  next_state, reward, done, info = env.step(action)\n",
    "  reward = np.random.normal(1, 1)\n",
    "  if done:\n",
    "    next_state = None\n",
    "\n",
    "  # This result is stored as an agent experience. The\n",
    "  # experiences are stored in a memory object attached to the agent. This\n",
    "  # object handles storage and access of batches of previous experiences\n",
    "  # for training the networks which replace the tabular approximation\n",
    "  # function from the Q-learning algorithm. \n",
    "  self.experience_buffer.add_experiences((state, action, reward, next_state))\n",
    "\n",
    "  # We compute the loss and update the approximation function. Complete\n",
    "  # details of this function will be explained later.\n",
    "  loss = self.train(self.experience_buffer)\n",
    "  avg_loss += loss\n",
    "  state = next_state\n",
    "  steps += 1\n",
    "\n",
    "  # We also update the epsilon value, decaying it according to the 'lambda'\n",
    "  # hyperparameter.\n",
    "  eps = min_epsilon + (max_epsilon - min_epsilon) * math.exp(-lambda_ * steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see how the implementation of the DQN algorithm begins to \n",
    "differ from that of the tabular Q-learning algorithm by the introduction of the experience object which replaces the original table for storing state-action reward pairs. Because there are too many state-action pairs, rather than storing rewards in specific cells of a table, we store all collected memories and then batch these to iteratively train the network to approximate the expected reward and Q value based on this training data. Within the main approximation function update loop of the agent, we will access these memories to train the functions and ultimately the agent.\n",
    "\n",
    "The 'train' function implements the main update steps from the DQN algorithm, \n",
    "which will ultimately differ slightly from the DDQN algorithm by the number\n",
    "of integrated networks. During each step of the main training loop, after \n",
    "selecting an action we update the network by training it on a batch of \n",
    "experiences from the experience object. First we gather an appropriate batch, and\n",
    "then use the network to compute the values necessary for the Q-learning algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we estimate the Q value for the current state.\n",
    "Q_train = self.train_net(states)\n",
    "\n",
    "# Then the Q value for the next state.\n",
    "Q_train_prime = self.train_net(next_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the Q-learning algorithm, the value for the current state added \n",
    "to the prediction for next states will be used to update this state action pair.\n",
    "As we are doing this with a network, it is a bit more complicated than just \n",
    "looking up the appropriate values in a table and computing the new Q value. We must\n",
    "ultimately create a target value based on the batch of experiences which we will \n",
    "use to train the network to better approximate the value. \n",
    "\n",
    "With reference to the Q learning algorithm from before, we see that we have already implemented the first bullet points of the main training loop. The reward value is collected from the random sample of experiences:\n",
    "\n",
    "\n",
    "- **loop** untlil episode terminates\n",
    "    - Sample action $a_t$ from plolicy $\\pi(s_t)$\n",
    "    - Take action $a_t$ and observe reward $r_t$ and next state $s_{t+1}$\n",
    "    - $Q(s_t, a_t) \\gets Q(s_t, a_t) + \\alpha[r_t + \\gamma\\max\\limits_{a'}Q(s_{t+1},a') - Q(s_t, a_t)]$\n",
    "    \n",
    "    \n",
    "From the third bullet point, we want to update the state action pair based on \n",
    "a combination of the current and maximized next state values. In terms of the \n",
    "neural network, this means that instead of performing the equation above\n",
    "verbatim, we must first compute the maximized Q-value (the part inside of the brackets)\n",
    "to determine the expected reward of the current action and the following states.\n",
    "These will ultimately be used to compute the loss in order to train the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We begin with the training Q value from the equation below. This is the first\n",
    "# Q value after the arrow. We need to compute the rest to combine with this \n",
    "# value in order to determine the full network target value needed for the update.\n",
    "Q_train_target = Q_train.numpy()\n",
    "\n",
    "# First we set the rewards as the update we will ultimately feed to the network.\n",
    "updates = rewards\n",
    "\n",
    "# For training we want to use only 'valid' ids from the batch of experiences, \n",
    "# ie those without next state == 0.\n",
    "valid_idxs = np.array(next_states).sum(axis=1) != 0\n",
    "batch_idxs = np.arange(self.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we add to those rewards (the 'updates' variable), the rest of the info\n",
    "from inside the brackets: the maximized estimation of the next state Q value,\n",
    "tempered by the next-state discount factor. Note that we do not\n",
    "compute the difference between the next and current state, as this is undertaken\n",
    "during the backpropogation step of the network training. We also do not need\n",
    "to explicitly code the learning rate, alpha, because this is integrated into the \n",
    "optimizer functions from Keras. Here we use the default value from the Adam \n",
    "optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if self.algorithm == 'dqn':    \n",
    "  updates[valid_idxs] += self.gamma * np.amax(Q_train_prime.numpy()[valid_idxs, :], axis=1)\n",
    "\n",
    "# We then 'perform' the update by combining the original predicted value with the \n",
    "# weighted Q value for next states. Remember, we still do not have the difference\n",
    "# between the previous and next state values, so we will need to compute the loss\n",
    "# before we can update the network.\n",
    "Q_train_target[batch_idxs, actions] = updates\n",
    "\n",
    "# We compute the loss by having the network predict on the current state\n",
    "# and then observing the difference between the prediction and the update we \n",
    "# previously computed. This implements the final section of the bracketed equation.\n",
    "with tf.GradientTape() as tape:\n",
    "  output = self.train_net(states)\n",
    "  loss = self.mse(Q_train_target, output)\n",
    "  gradients = tape.gradient(loss, self.train_net.trainable_variables)\n",
    "\n",
    "# Now, the entire right portion of the equation is computed and we can update the\n",
    "# network based on the gradients.\n",
    "self.optimizer.apply_gradients(zip(gradients, self.train_net.trainable_variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform this update at every stage of the episode while loop, per the \n",
    "Q-learning algorithm. As the network accumulates experiences, given that it \n",
    "will train on a maximization of the Q values, the agent will gradually\n",
    "optimize to choose actions which result in the highest Q-values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from dqn_and_ddqn import ExperienceBuffer, Qnetwork, Agent\n",
    "\n",
    "# After instantiating our experience ...\n",
    "dqn_experience = ExperienceBuffer(max_experiences, min_experiences)\n",
    "\n",
    "# The network ...\n",
    "dqn_train_network = Qnetwork(num_states, hidden_units, num_actions)\n",
    "\n",
    "# And organizing everything into our agent ...\n",
    "dqn_agent = Agent(train_net=dqn_train_network, gamma=gamma, batch_size=batch_size, \n",
    "                  num_actions=num_actions, algorithm='dqn', experience=dqn_experience)\n",
    "\n",
    "# We run the main training loop to see how the algorithm performs at each step.\n",
    "dqn_agent.learn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As visible in the graph below, the neural network implementation of the Q-learning algorithm effectively learns to perform within the more-complicated OpenAI gym environment. Just like in the graphs from the tabular implementation, the agent learns to increase the rewards by training on batches of previous experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_agent.visualize_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to test the agent without further training, we can see specifically how the policy influences performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "97oCkd1dVFej"
   },
   "outputs": [],
   "source": [
    "from utils import wrap_env, make_video, show_video\n",
    "\n",
    "make_video(env, dqn_agent)\n",
    "env.close()\n",
    "show_video()\n",
    "\n",
    "dqn_train_network.save_weights('DQN_cart_pole.h5')\n",
    "weights_file = drive.CreateFile({'title' : 'DQN_cart_pole.h5'})\n",
    "weights_file.SetContentFile('DQN_cart_pole.h5')\n",
    "weights_file.Upload()\n",
    "drive.CreateFile({'id': weights_file.get('id')})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "38WQ3YLZWS8R"
   },
   "source": [
    "### Deep Reinforcement Learning with Double Deep Q Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gOYA58pgwNRc"
   },
   "source": [
    "\n",
    "The DDQN algorithm is the Double Q version of the DQN algorithm. In their 2015 paper, Hasselt et al. investigated the performance of the aforementioned DQN algorithm and showed that it also produces systematic non-uniform overestimations which lead to suboptimal agent policies. Since the introduction of the Q-learning algorithm in 1989, it was known that this algorithm sometimes led to suboptimal policies and overestimated values, but the exact causes or solutions for these inadequacies were unknown (Watkins, 1989). As early as 1993, it was hypothesized that inflexible function approximations and noise led to such overestimations, and in their paper, Hasselt et al experimentally demonstrate the truth of these hypotheses in relation to the network update procedures of the DQN algorithm (Thrun & Schwartz, 1993).\n",
    "\n",
    "Because the DQN algorithm ultimately trains only one Q-value approximation network, the algorithm reproduces same unnecessary conjunction of action selection and evaluation from Q learning, which, if decoupled, could lead to a uniform estimation distribution which would mitigate the effects of overestimation. Effectively, if all Q-values are equally overestimated ie in a normal distribution, then overestimation becomes a non-issue for the training protocol, and this uniformity can be achieved by the introduction of an additional Q-value into the original Q/DQN-learning algorithm. In the DDQN algorithm, estimation uniformity is achieved by requisitioning the target network's parameters and, rather than simply copying them from the online network, using them in conjunction with the online network to randomly update either one of two sets of parameters, θ or θ', representing the parameters of the online or target network, respectively. As Hasselt et al state, \"... two value functions are learned by assigning experience randomly to update one of the two value functions, such that there are two sets of weights ...\" (Hasselt et al., 2015).\n",
    "\n",
    "It is worth noting that Hasselt's proposal for implementing the DDQN algorithm still involves only two networks rather than four, which could be implied given that the DQN algorithm involves two networks and implements only a single Q-value. In Hasselt's estimation, leveraging the DQN target network for representing the second Q-value function is \"the minimal possible change to DQN towards Double Q-Learning. The goal is to get most of the benefit of Double Q-Learning, while keeping the rest of the DQN algorithm intact for a fair comparison\" (Hasselt et al, 2015, p4). Thus, in our implementation we stick with the two network structure suggested by Hasselt et al, conceding that other interpretations of the DDQN algorithm are possible.\n",
    "\n",
    "By randomly selecting updates for the Q and Q' approximation functions, the Double Q-learning algorithm is able to remove the non-uniform estimation biases seen in the single-Q-value learning algorithms. Hasselt et al demonstrate these results in the following graph, which shows that when estimating Q values with a single approximation function, as the number of actions increases, the estimations (red) systematically increase. However, when an additional approximation function (blue) is introduced, there appears no overestimation bias as a function of increasing number of actions, suggesting that the suboptimal policy training is mitigated by the additional value. In our project we intended to replicate these results by implementing the Double Deep Q learning algorithm, which uses two neural networks for approximating the separate Q-values.\n",
    "\n",
    "\n",
    "![alt text](http://drive.google.com/uc?export=view&id=1SNdOFx9PpIeRewXuWarM6CqiQXbkRcXj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our implementation, the DDQN agent uses the same 'Agent' class as for the DQN\n",
    "agent, but with a few minor tweaks. As mentioned, because we are using two \n",
    "approximation functions, we must pass two networks to the agent, a training \n",
    "and a target network which will be used to separate action selection and \n",
    "evaluation and will ultimately be combined in the equation used to update\n",
    "the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the target network, we must compute the next state using this network, thereby\n",
    "# achieving the separation between selection and evaluation proposed by Hasselt. \n",
    "elif self.algorithm == 'ddqn':\n",
    "  A_prime = np.argmax(Q_train_prime.numpy(), axis=1)\n",
    "  Q_target = self.target_net(next_states)\n",
    "  # The update equation then, looks very similar to that from DQN, with the key\n",
    "  # difference that not the target Q values are integrated as the next state\n",
    "  # evaluation used for the update. \n",
    "  updates[valid_idxs] += self.gamma * Q_target.numpy()[batch_idxs[valid_idxs], A_prime[valid_idxs]]\n",
    "\n",
    "# We then use the target network to update the training network. Here we make a \n",
    "# 'soft' copy of the network according to the parameter tau. At every iteration\n",
    "# of the update algorithm, we adjust the training network according to the target.\n",
    "if self.algorithm == 'ddqn':\n",
    "  for t, e in zip(self.target_net.trainable_variables, self.train_net.trainable_variables):\n",
    "    t.assign(t * (1 - tau) + e * tau)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rudFdvd5-CpI",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ddqn_experience = ExperienceBuffer(max_experiences, min_experiences)\n",
    "ddqn_train_network = Qnetwork(num_states, hidden_units, num_actions)\n",
    "ddqn_target_network = Qnetwork(num_states, hidden_units, num_actions)\n",
    "ddqn_agent = Agent(train_net=dqn_train_network, target_net=ddqn_target_network, \n",
    "                   gamma=gamma, batch_size=batch_size, num_actions=num_actions, \n",
    "                   algorithm='ddqn', experience=ddqn_experience)\n",
    "\n",
    "ddqn_agent.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vlrxh9tgSdFH"
   },
   "outputs": [],
   "source": [
    "ddqn_agent.visualize_training()\n",
    "make_video(env, ddqn_agent)\n",
    "env.close()\n",
    "show_video()\n",
    "\n",
    "ddqn_train_network.save_weights('DDQN_train_cart_pole.h5')\n",
    "ddqn_target_network.save_weights('DDQN_target_cart_pole.h5')\n",
    "weights_file = drive.CreateFile({'title' : 'DQN_cart_pole.h5'})\n",
    "weights_file.SetContentFile('DQN_cart_pole.h5')\n",
    "weights_file.Upload()\n",
    "drive.CreateFile({'id': weights_file.get('id')})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7WMgQOh_-NPl"
   },
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EcleKm8t-QqS"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16,5))\n",
    "loss_ax = fig.add_subplot(1,2,1)\n",
    "plt.plot(dqn_agent.train_steps, dqn_agent.train_losses, label='DQN')\n",
    "plt.plot(ddqn_agent.train_steps,ddqn_agent.train_losses, label='DDQN')\n",
    "loss_ax.title.set_text('Loss plot')\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "loss_ax.set_xlabel('Episodes')\n",
    "loss_ax.set_ylabel('Loss')\n",
    "\n",
    "rewards_ax = fig.add_subplot(1,2,2)\n",
    "plt.plot(dqn_agent.train_steps, dqn_agent.train_rewards, label='DQN')\n",
    "plt.plot(ddqn_agent.train_steps, ddqn_agent.train_rewards, label='DDQN')\n",
    "rewards_ax.title.set_text('Reward plot')\n",
    "plt.legend()\n",
    "\n",
    "rewards_ax.set_xlabel('Episode')\n",
    "rewards_ax.set_ylabel('Rewards')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Visualization of results\n",
    "# Comparison between DQN and DDQN\n",
    "# Make a graph like in the 2015 paper.\n",
    "# Add video file from producing agent.\n",
    "# Showing how the overestimation is resolved.\n",
    "# Make more modular - execution, visualization.\n",
    "# Plot DQN and DDQN in same graph.\n",
    "# Run test for 100 iterations and compare.\n",
    "# Save models and submit such that they can rerun it easily.\n",
    "\n",
    "#################TODO######################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K_0oMxBttlcY"
   },
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S0g5rlLutoJY"
   },
   "source": [
    "* Hasselt, Hado. (2010). Double Q-learning.. 2613-2621. \n",
    "\n",
    "* Hasselt, Hado & Guez, Arthur & Silver, David. (2015). Deep Reinforcement Learning with Double Q-learning. \n",
    "\n",
    "* Mnih, Volodymyr & Kavukcuoglu, Koray & Silver, David & Rusu, Andrei & Veness, Joel & Bellemare, Marc & Graves, Alex & Riedmiller, Martin & Fidjeland, Andreas & Ostrovski, Georg & Petersen, Stig & Beattie, Charles & Sadik, Amir & Antonoglou, Ioannis & King, Helen & Kumaran, Dharshan & Wierstra, Daan & Legg, Shane & Hassabis, Demis. (2015). Human-level control through deep reinforcement learning. Nature. 518. 529-33. 10.1038/nature14236. \n",
    "\n",
    "* Watkins, Christopher. (1989). Learning From Delayed Rewards. PhD Thesis. University of Cambridge.\n",
    "\n",
    "* Thrun, Sebastian & Schwartz, Anton. (1993). Issues in Using Function Approximation for Reinforcement Learning. In Proceedings of the Fourth Connectionist Models Summer School. Erlbaum.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "bwwvaDBrtTpq",
    "38WQ3YLZWS8R",
    "7WMgQOh_-NPl",
    "K_0oMxBttlcY"
   ],
   "name": "Group 22 Final Project.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
